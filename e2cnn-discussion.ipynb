{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bottom-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import e2cnn\n",
    "import scipy\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from e2cnn import gspaces\n",
    "from mlxtend.data import loadlocal_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "friendly-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset class\n",
    "class MnistDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, mode, transform=None, rotation=None):\n",
    "        assert mode in ['train', 'test']\n",
    "        data_dir = '/home/zhuokai/Desktop/nvme1n1p1/Data/MNIST/original'\n",
    "\n",
    "        if mode == 'train':\n",
    "            images_path = os.path.join(data_dir, 'train-images-idx3-ubyte')\n",
    "            labels_path = os.path.join(data_dir, 'train-labels-idx1-ubyte')\n",
    "        elif mode == 'test':\n",
    "            self.rotation = rotation\n",
    "            images_path = os.path.join(data_dir, 't10k-images-idx3-ubyte')\n",
    "            labels_path = os.path.join(data_dir, 't10k-labels-idx1-ubyte')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images, self.labels = loadlocal_mnist(images_path=images_path,\n",
    "                                                   labels_path=labels_path)\n",
    "\n",
    "        self.images = self.images.reshape(-1, 28, 28).astype(np.float32)\n",
    "        self.labels = self.labels.astype(np.int64)\n",
    "        self.num_samples = len(self.labels)\n",
    "\n",
    "        # normalization and conversion\n",
    "        self.to_tensor = torchvision.transforms.ToTensor()\n",
    "        self.normalize = torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.images[index], self.labels[index]\n",
    "        image = Image.fromarray(image)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            image = self.to_tensor(np.array(image))\n",
    "            image = self.normalize(image)\n",
    "\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "annual-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation equivariant network using e2cnn\n",
    "class Rot_Eqv_Net_MNIST(torch.nn.Module):\n",
    "    def __init__(self, image_size, num_rotation, n_classes=10):\n",
    "\n",
    "        super(Rot_Eqv_Net_MNIST, self).__init__()\n",
    "\n",
    "        # the model is equivariant under rotations by 45 degrees, modelled by C8\n",
    "        self.r2_act = gspaces.Rot2dOnR2(N=num_rotation)\n",
    "\n",
    "        # the input image is a scalar field, corresponding to the trivial representation\n",
    "        in_type = e2cnn.nn.FieldType(self.r2_act, [self.r2_act.trivial_repr])\n",
    "\n",
    "        # we store the input type for wrapping the images into a geometric tensor during the forward pass\n",
    "        self.input_type = in_type\n",
    "\n",
    "        # convolution 1\n",
    "        # first specify the output type of the convolutional layer\n",
    "        # we choose 24 feature fields, each transforming under the regular representation of C8\n",
    "        out_type = e2cnn.nn.FieldType(self.r2_act, 24*[self.r2_act.regular_repr])\n",
    "        if image_size != None:\n",
    "            self.block1 = e2cnn.nn.SequentialModule(\n",
    "                e2cnn.nn.MaskModule(in_type, image_size[0], margin=image_size[0]-28),\n",
    "                e2cnn.nn.R2Conv(in_type, out_type, kernel_size=7, padding=2, bias=False),\n",
    "                e2cnn.nn.InnerBatchNorm(out_type),\n",
    "                e2cnn.nn.ReLU(out_type, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block1 = e2cnn.nn.SequentialModule(\n",
    "                e2cnn.nn.MaskModule(in_type, 29, margin=1),\n",
    "                e2cnn.nn.R2Conv(in_type, out_type, kernel_size=7, padding=2, bias=False),\n",
    "                e2cnn.nn.InnerBatchNorm(out_type),\n",
    "                e2cnn.nn.ReLU(out_type, inplace=True)\n",
    "            )\n",
    "\n",
    "        # convolution 2\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block1.out_type\n",
    "        # the output type of the second convolution layer are 48 regular feature fields of C8\n",
    "        out_type = e2cnn.nn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n",
    "        self.block2 = e2cnn.nn.SequentialModule(\n",
    "            e2cnn.nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            e2cnn.nn.InnerBatchNorm(out_type),\n",
    "            e2cnn.nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool1 = e2cnn.nn.SequentialModule(\n",
    "            e2cnn.nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        )\n",
    "\n",
    "        # convolution 3\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block2.out_type\n",
    "        # the output type of the third convolution layer are 48 regular feature fields of C8\n",
    "        out_type = e2cnn.nn.FieldType(self.r2_act, 48*[self.r2_act.regular_repr])\n",
    "        self.block3 = e2cnn.nn.SequentialModule(\n",
    "            e2cnn.nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            e2cnn.nn.InnerBatchNorm(out_type),\n",
    "            e2cnn.nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "\n",
    "        # convolution 4\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block3.out_type\n",
    "        # the output type of the fourth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = e2cnn.nn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n",
    "        self.block4 = e2cnn.nn.SequentialModule(\n",
    "            e2cnn.nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            e2cnn.nn.InnerBatchNorm(out_type),\n",
    "            e2cnn.nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool2 = e2cnn.nn.SequentialModule(\n",
    "            e2cnn.nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        )\n",
    "\n",
    "        # convolution 5\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block4.out_type\n",
    "        # the output type of the fifth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = e2cnn.nn.FieldType(self.r2_act, 96*[self.r2_act.regular_repr])\n",
    "        self.block5 = e2cnn.nn.SequentialModule(\n",
    "            e2cnn.nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, bias=False),\n",
    "            e2cnn.nn.InnerBatchNorm(out_type),\n",
    "            e2cnn.nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "\n",
    "        # convolution 6\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = self.block5.out_type\n",
    "        # the output type of the sixth convolution layer are 64 regular feature fields of C8\n",
    "        out_type = e2cnn.nn.FieldType(self.r2_act, 64*[self.r2_act.regular_repr])\n",
    "        self.block6 = e2cnn.nn.SequentialModule(\n",
    "            e2cnn.nn.R2Conv(in_type, out_type, kernel_size=5, padding=1, bias=False),\n",
    "            e2cnn.nn.InnerBatchNorm(out_type),\n",
    "            e2cnn.nn.ReLU(out_type, inplace=True)\n",
    "        )\n",
    "        self.pool3 = e2cnn.nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=1, padding=0)\n",
    "\n",
    "        self.gpool = e2cnn.nn.GroupPooling(out_type)\n",
    "\n",
    "        # number of output channels\n",
    "        # c = self.gpool.out_type.size\n",
    "        c = 6400\n",
    "\n",
    "        # Fully Connected\n",
    "        self.fully_net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(c, 64),\n",
    "            torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.ELU(inplace=True),\n",
    "            torch.nn.Linear(64, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        # wrap the input tensor in a GeometricTensor\n",
    "        # (associate it with the input type)\n",
    "        x = e2cnn.nn.GeometricTensor(input, self.input_type)\n",
    "\n",
    "        # apply each equivariant block\n",
    "\n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "\n",
    "        # pool over the spatial dimensions\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # pool over the group\n",
    "        x = self.gpool(x)\n",
    "\n",
    "        # unwrap the output GeometricTensor\n",
    "        # (take the Pytorch tensor and discard the associated representation)\n",
    "        x = x.tensor\n",
    "\n",
    "        # classify with the final fully connected layers)\n",
    "        x = self.fully_net(x.reshape(x.shape[0], -1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "motivated-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print iterations progress\n",
    "def print_progress_bar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "absolute-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train each epoch\n",
    "def train(model, device, train_loader, optimizer):\n",
    "    all_batch_losses = []\n",
    "    model.train()\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        batch_time_start = time.time()\n",
    "        # move data to GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(torch.nn.functional.softmax(output).data[0])\n",
    "        # negative log likelihood loss.\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        all_batch_losses.append(loss.item())\n",
    "        optimizer.step()\n",
    "        batch_time_end = time.time()\n",
    "        batch_time_cost = batch_time_end - batch_time_start\n",
    "\n",
    "        print_progress_bar(iteration=batch_idx+1,\n",
    "                            total=len(train_loader),\n",
    "                            prefix=f'Train batch {batch_idx+1}/{len(train_loader)},',\n",
    "                            suffix='%s: %.3f, time: %.2f' % ('CE loss', all_batch_losses[-1], batch_time_cost),\n",
    "                            length=50)\n",
    "\n",
    "    # return the averaged batch loss\n",
    "    return np.mean(all_batch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "corresponding-crystal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "def val(model, device, val_loader):\n",
    "    model.eval()\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    all_batch_losses = []\n",
    "    num_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            batch_time_start = time.time()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            all_batch_losses.append(loss.item())\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            num_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            batch_time_end = time.time()\n",
    "            batch_time_cost = batch_time_end - batch_time_start\n",
    "\n",
    "            print_progress_bar(iteration=batch_idx+1,\n",
    "                                total=len(val_loader),\n",
    "                                prefix=f'Val batch {batch_idx+1}/{len(val_loader)},',\n",
    "                                suffix='%s: %.3f, time: %.2f' % ('CE loss', all_batch_losses[-1], batch_time_cost),\n",
    "                                length=50)\n",
    "\n",
    "    # return the averaged batch loss\n",
    "    return np.mean(all_batch_losses), num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "secure-implementation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3 GPUs available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhuokai/anaconda3/envs/vis/lib/python3.7/site-packages/e2cnn/nn/modules/r2_conv/basisexpansion_singleblock.py:61: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "  sampled_basis = sampled_basis[mask, ...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting epoch 1/10\n",
      "Train batch 782/782, |██████████████████████████████████████████████████| 100.0% CE loss: 1.390, time: 0.09\n",
      "Val batch 100/100, |██████████████████████████████████████████████████| 100.0% CE loss: 1.265, time: 0.07\n",
      "\n",
      "Validation set: Average loss: 1.0901, Accuracy: 6178/10000 (62%)\n",
      "\n",
      "\n",
      " Starting epoch 2/10\n",
      "Train batch 782/782, |██████████████████████████████████████████████████| 100.0% CE loss: 0.853, time: 0.09\n",
      "Val batch 100/100, |██████████████████████████████████████████████████| 100.0% CE loss: 0.988, time: 0.07\n",
      "\n",
      "Validation set: Average loss: 0.9770, Accuracy: 6343/10000 (63%)\n",
      "\n",
      "\n",
      " Starting epoch 3/10\n",
      "Train batch 782/782, |██████████████████████████████████████████████████| 100.0% CE loss: 1.179, time: 0.10\n",
      "Val batch 100/100, |██████████████████████████████████████████████████| 100.0% CE loss: 1.063, time: 0.07\n",
      "\n",
      "Validation set: Average loss: 0.9386, Accuracy: 6561/10000 (66%)\n",
      "\n",
      "\n",
      " Starting epoch 4/10\n",
      "Train batch 782/782, |██████████████████████████████████████████████████| 100.0% CE loss: 0.811, time: 0.09\n",
      "Val batch 100/100, |██████████████████████████████████████████████████| 100.0% CE loss: 0.964, time: 0.07\n",
      "\n",
      "Validation set: Average loss: 0.9119, Accuracy: 6630/10000 (66%)\n",
      "\n",
      "\n",
      " Starting epoch 5/10\n",
      "Train batch 782/782, |██████████████████████████████████████████████████| 100.0% CE loss: 1.297, time: 0.09\n",
      "Val batch 100/100, |██████████████████████████████████████████████████| 100.0% CE loss: 0.961, time: 0.07\n",
      "\n",
      "Validation set: Average loss: 0.8915, Accuracy: 6650/10000 (66%)\n",
      "\n",
      "\n",
      " Starting epoch 6/10\n",
      "Train batch 782/782, |██████████████████████████████████████████████████| 100.0% CE loss: 0.943, time: 0.09\n",
      "Val batch 100/100, |██████████████████████████████████████████████████| 100.0% CE loss: 0.968, time: 0.07\n",
      "\n",
      "Validation set: Average loss: 0.8630, Accuracy: 6729/10000 (67%)\n",
      "\n",
      "\n",
      " Starting epoch 7/10\n",
      "Train batch 782/782, |██████████████████████████████████████████████████| 100.0% CE loss: 1.350, time: 0.09\n",
      "Val batch 100/100, |██████████████████████████████████████████████████| 100.0% CE loss: 0.809, time: 0.07\n",
      "\n",
      "Validation set: Average loss: 0.8696, Accuracy: 6793/10000 (68%)\n",
      "\n",
      "\n",
      " Starting epoch 8/10\n",
      "Train batch 782/782, |██████████████████████████████████████████████████| 100.0% CE loss: 0.647, time: 0.09\n",
      "Val batch 100/100, |██████████████████████████████████████████████████| 100.0% CE loss: 0.860, time: 0.07\n",
      "\n",
      "Validation set: Average loss: 0.8561, Accuracy: 6827/10000 (68%)\n",
      "\n",
      "\n",
      " Starting epoch 9/10\n",
      "Train batch 782/782, |██████████████████████████████████████████████████| 100.0% CE loss: 0.777, time: 0.09\n",
      "Val batch 100/100, |██████████████████████████████████████████████████| 100.0% CE loss: 0.573, time: 0.07\n",
      "\n",
      "Validation set: Average loss: 0.8735, Accuracy: 6721/10000 (67%)\n",
      "\n",
      "\n",
      " Starting epoch 10/10\n",
      "Train batch 782/782, |██████████████████████████████████████████████████| 100.0% CE loss: 0.745, time: 0.09\n",
      "Val batch 100/100, |██████████████████████████████████████████████████| 100.0% CE loss: 0.981, time: 0.07\n",
      "\n",
      "Validation set: Average loss: 0.8480, Accuracy: 6825/10000 (68%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kwargs for train and val dataloader\n",
    "train_batch_size = 64\n",
    "val_batch_size = 100\n",
    "train_kwargs = {'batch_size': train_batch_size}\n",
    "val_kwargs = {'batch_size': val_batch_size}\n",
    "if torch.cuda.is_available():\n",
    "    # additional cuda kwargs\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    val_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    # device set up\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print('\\n', torch.cuda.device_count(), 'GPUs available')\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "image_size = (64, 64)\n",
    "transform = torchvision.transforms.Compose([\n",
    "                    # transform includes padding (right and bottom) to image_size and totensor\n",
    "                    torchvision.transforms.Pad((0, 0, image_size[1]-28, image_size[0]-28), fill=0, padding_mode='constant'),\n",
    "                ])\n",
    "\n",
    "# split into train and validation dataset\n",
    "dataset = MnistDataset(mode='train', transform=transform)\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [50000, 10000])\n",
    "# pytorch data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_data, **train_kwargs)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, **val_kwargs)\n",
    "\n",
    "starting_epoch = 0\n",
    "model = Rot_Eqv_Net_MNIST(image_size=image_size, num_rotation=8).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
    "\n",
    "# train the model\n",
    "num_epoch = 10\n",
    "all_epoch_train_losses = []\n",
    "all_epoch_val_losses = []\n",
    "for epoch in range(starting_epoch, starting_epoch+num_epoch):\n",
    "    print(f'\\n Starting epoch {epoch+1}/{starting_epoch+num_epoch}')\n",
    "    epoch_start_time = time.time()\n",
    "    cur_epoch_train_loss = train(model, device, train_loader, optimizer)\n",
    "    all_epoch_train_losses.append(cur_epoch_train_loss)\n",
    "    cur_epoch_val_loss, num_correct = val(model, device, val_loader)\n",
    "    all_epoch_val_losses.append(cur_epoch_val_loss)\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time_cost = epoch_end_time - epoch_start_time\n",
    "\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            cur_epoch_val_loss, num_correct, len(val_loader.dataset),\n",
    "            100. * num_correct / len(val_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-dubai",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
